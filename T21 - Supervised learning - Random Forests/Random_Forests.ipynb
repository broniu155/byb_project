{"cells":[{"cell_type":"code","execution_count":335,"metadata":{"id":"mFPFeEu69axo"},"outputs":[],"source":["#import libraries\n","import pandas as pd\n","\n","# for visualisation\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#import sci-kit\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import tree\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, classification_report"]},{"cell_type":"code","execution_count":336,"metadata":{"id":"e9C-68Ij9ayD","outputId":"cb08f206-100d-4a4c-f87d-13f2aec390cc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  "]},"execution_count":336,"metadata":{},"output_type":"execute_result"}],"source":["titanic_df = pd.read_csv(\"titanic.csv\")\n","titanic_df.head()"]},{"cell_type":"code","execution_count":337,"metadata":{"id":"CBV7Mx939ayO","outputId":"4e34ffef-0da5-49e5-d52c-379240000dc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 12 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   PassengerId  891 non-null    int64  \n"," 1   Survived     891 non-null    int64  \n"," 2   Pclass       891 non-null    int64  \n"," 3   Name         891 non-null    object \n"," 4   Sex          891 non-null    object \n"," 5   Age          714 non-null    float64\n"," 6   SibSp        891 non-null    int64  \n"," 7   Parch        891 non-null    int64  \n"," 8   Ticket       891 non-null    object \n"," 9   Fare         891 non-null    float64\n"," 10  Cabin        204 non-null    object \n"," 11  Embarked     889 non-null    object \n","dtypes: float64(2), int64(5), object(5)\n","memory usage: 83.7+ KB\n"]}],"source":["titanic_df.info()"]},{"cell_type":"markdown","metadata":{},"source":["### EDA Process"]},{"cell_type":"code","execution_count":338,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 8 columns):\n"," #   Column    Non-Null Count  Dtype  \n","---  ------    --------------  -----  \n"," 0   Survived  891 non-null    int64  \n"," 1   Pclass    891 non-null    int64  \n"," 2   Sex       891 non-null    object \n"," 3   Age       891 non-null    int32  \n"," 4   SibSp     891 non-null    int64  \n"," 5   Parch     891 non-null    int64  \n"," 6   Fare      891 non-null    float64\n"," 7   Embarked  891 non-null    object \n","dtypes: float64(1), int32(1), int64(4), object(2)\n","memory usage: 52.3+ KB\n"]}],"source":["# To make changes create new subset of the original dataset\n","titanic_clean_dt = titanic_df.copy()\n","\n","# Fill the NaN and blanks to 0 and convert the Age type to integer \n","titanic_clean_dt['Age'] = titanic_clean_dt['Age'].fillna(0).astype(int)\n","\n","# Input missing values in 'Embarked' with the most frequent value (mode)\n","mode_embarked = titanic_clean_dt['Embarked'].mode()[0]\n","titanic_clean_dt['Embarked'] = titanic_clean_dt['Embarked'].fillna(mode_embarked)\n","\n","# Delete the Cabin, Ticket and Name column from the Dataset as not required\n","titanic_clean_dt.drop([\"PassengerId\",\"Cabin\",\"Ticket\",\"Name\"], axis=1, inplace=True)\n","\n","titanic_clean_dt.info()"]},{"cell_type":"markdown","metadata":{"id":"zFZ50hDg9aye"},"source":["### One-Hot Encoding\n","One-hot encoding is a technique used to ensure that categorical variables are better represented in the machine. Let's take a look at the \"Sex\" column"]},{"cell_type":"code","execution_count":339,"metadata":{"id":"zxl62Q0-9ay1","outputId":"1b2e62ad-3707-424d-fc21-26d413fc9221"},"outputs":[{"data":{"text/plain":["array(['male', 'female'], dtype=object)"]},"execution_count":339,"metadata":{},"output_type":"execute_result"}],"source":["titanic_clean_dt[\"Sex\"].unique()"]},{"cell_type":"markdown","metadata":{"id":"sXIAVO4Z9ay8"},"source":["Machine Learning classifiers don't know how to handle strings. As a result, you need to convert it into a categorical representation. There are two main ways to go about this:\n","\n","Label Encoding: Assigning, for example, 0 for \"male\" and 1 for \"female\". The problem here is it intrinsically makes one category \"larger than\" the other category.\n","\n","One-hot encoding: Assigning, for example, [1, 0] for \"male\" and [0, 1] for female. In this case, you have an array of size (n_categories,) and you represent a 1 in the correct index, and 0 elsewhere. In Pandas, this would show as extra columns. For example, rather than having a \"Sex\" column, it would be a \"Sex_male\" and \"Sex_female\" column. Then, if the person is male, it would simply show as a 1 in the \"Sex_male\" column and a 0 in the \"Sex_female\" column.\n","\n","There is a nice and easy method that does this in pandas: get_dummies()"]},{"cell_type":"code","execution_count":340,"metadata":{"id":"hMLpI7IP9azC","outputId":"8973bd4a-7ecd-4abf-edda-ef096ce958d7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Fare</th>\n","      <th>Embarked</th>\n","      <th>Sex_female</th>\n","      <th>Sex_male</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>22</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>38</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>S</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>S</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Survived  Pclass  Age  SibSp  Parch     Fare Embarked  Sex_female  Sex_male\n","0         0       3   22      1      0   7.2500        S       False      True\n","1         1       1   38      1      0  71.2833        C        True     False\n","2         1       3   26      0      0   7.9250        S        True     False\n","3         1       1   35      1      0  53.1000        S        True     False\n","4         0       3   35      0      0   8.0500        S       False      True"]},"execution_count":340,"metadata":{},"output_type":"execute_result"}],"source":["titanic_clean_dt = pd.get_dummies(titanic_clean_dt, prefix=\"Sex\", columns=[\"Sex\"])\n","titanic_clean_dt.head()"]},{"cell_type":"markdown","metadata":{"id":"4b3ItGkb9azJ"},"source":["Now, we do the same to the \"Embarked\" column."]},{"cell_type":"code","execution_count":341,"metadata":{"id":"5YEGpK3Y9azM","outputId":"4d16f7a8-987d-48ea-b043-8d7391e0491a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 11 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Survived    891 non-null    int64  \n"," 1   Pclass      891 non-null    int64  \n"," 2   Age         891 non-null    int32  \n"," 3   SibSp       891 non-null    int64  \n"," 4   Parch       891 non-null    int64  \n"," 5   Fare        891 non-null    float64\n"," 6   Sex_female  891 non-null    bool   \n"," 7   Sex_male    891 non-null    bool   \n"," 8   Embarked_C  891 non-null    bool   \n"," 9   Embarked_Q  891 non-null    bool   \n"," 10  Embarked_S  891 non-null    bool   \n","dtypes: bool(5), float64(1), int32(1), int64(4)\n","memory usage: 42.8 KB\n"]}],"source":["titanic_clean_dt = pd.get_dummies(titanic_clean_dt, prefix=\"Embarked\", columns=[\"Embarked\"])\n","titanic_clean_dt.head()\n","titanic_clean_dt.info()"]},{"cell_type":"markdown","metadata":{},"source":["### Modeling"]},{"cell_type":"code","execution_count":342,"metadata":{},"outputs":[{"data":{"text/plain":["(891, 10)"]},"execution_count":342,"metadata":{},"output_type":"execute_result"}],"source":["# Define features and target variable\n","X = titanic_clean_dt.drop(\"Survived\", axis=1)\n","y = titanic_clean_dt[\"Survived\"]\n","X.shape"]},{"cell_type":"code","execution_count":343,"metadata":{},"outputs":[],"source":["# Split the data into training (60%), development (20%), and test (20%) sets\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n","X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Standardize the features (optional but often useful)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_dev_scaled = scaler.transform(X_dev)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Comparing models"]},{"cell_type":"code","execution_count":344,"metadata":{},"outputs":[],"source":["# Train and evaluate models Function for implement to all models\n","def train_and_evaluate(model, X_train, X_dev, y_train, y_dev, name):\n","    model.fit(X_train, y_train)\n","    y_train_pred = model.predict(X_train)\n","    y_dev_pred = model.predict(X_dev)\n","    \n","    train_accuracy = accuracy_score(y_train, y_train_pred)\n","    dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n","    train_report = classification_report(y_train, y_train_pred)\n","    dev_report = classification_report(y_dev, y_dev_pred)\n","    \n","    print(f\"Training Set Accuracy for {name}: {train_accuracy}\")\n","    print(f\"Training Set Classification Report for {name}:\")\n","    print(train_report)\n","    \n","    print(f\"Development Set Accuracy for {name}: {dev_accuracy}\")\n","    print(f\"Development Set Classification Report for {name}:\")\n","    print(dev_report)\n","    \n","    return\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train models"]},{"cell_type":"code","execution_count":345,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Set Accuracy for Decision Tree: 0.9831460674157303\n","Training Set Classification Report for Decision Tree:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      1.00      0.99       333\n","           1       1.00      0.96      0.98       201\n","\n","    accuracy                           0.98       534\n","   macro avg       0.99      0.98      0.98       534\n","weighted avg       0.98      0.98      0.98       534\n","\n","Development Set Accuracy for Decision Tree: 0.7584269662921348\n","Development Set Classification Report for Decision Tree:\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.84      0.82       113\n","           1       0.69      0.62      0.65        65\n","\n","    accuracy                           0.76       178\n","   macro avg       0.74      0.73      0.73       178\n","weighted avg       0.75      0.76      0.76       178\n","\n"]}],"source":["# Train a single Decision Tree model\n","dt_model = DecisionTreeClassifier(random_state=42)\n","train_and_evaluate(dt_model, X_train_scaled, X_dev_scaled, y_train, y_dev, \"Decision Tree\")\n"]},{"cell_type":"code","execution_count":346,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Set Accuracy for Bagging Classifier: 0.9831460674157303\n","Training Set Classification Report for Bagging Classifier:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      1.00      0.99       333\n","           1       1.00      0.96      0.98       201\n","\n","    accuracy                           0.98       534\n","   macro avg       0.99      0.98      0.98       534\n","weighted avg       0.98      0.98      0.98       534\n","\n","Development Set Accuracy for Bagging Classifier: 0.7584269662921348\n","Development Set Classification Report for Bagging Classifier:\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.83      0.81       113\n","           1       0.68      0.63      0.66        65\n","\n","    accuracy                           0.76       178\n","   macro avg       0.74      0.73      0.73       178\n","weighted avg       0.76      0.76      0.76       178\n","\n"]}],"source":["# Bagging model\n","bagging_model = BaggingClassifier(estimator=dt_model, n_estimators=100, random_state=42)\n","train_and_evaluate(bagging_model, X_train_scaled, X_dev_scaled, y_train, y_dev, \"Bagging Classifier\")\n"]},{"cell_type":"code","execution_count":347,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Set Accuracy for Random Forest: 0.9831460674157303\n","Training Set Classification Report for Random Forest:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      1.00      0.99       333\n","           1       1.00      0.96      0.98       201\n","\n","    accuracy                           0.98       534\n","   macro avg       0.99      0.98      0.98       534\n","weighted avg       0.98      0.98      0.98       534\n","\n","Development Set Accuracy for Random Forest: 0.7808988764044944\n","Development Set Classification Report for Random Forest:\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.85      0.83       113\n","           1       0.72      0.66      0.69        65\n","\n","    accuracy                           0.78       178\n","   macro avg       0.77      0.76      0.76       178\n","weighted avg       0.78      0.78      0.78       178\n","\n"]}],"source":["# Random Forest model\n","rf_classifier = RandomForestClassifier(random_state=42)\n","train_and_evaluate(rf_classifier, X_train_scaled, X_dev_scaled, y_train, y_dev, \"Random Forest\")\n"]},{"cell_type":"code","execution_count":348,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Set Accuracy for Gradient Boosting: 0.9044943820224719\n","Training Set Classification Report for Gradient Boosting:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.98      0.93       333\n","           1       0.96      0.78      0.86       201\n","\n","    accuracy                           0.90       534\n","   macro avg       0.92      0.88      0.89       534\n","weighted avg       0.91      0.90      0.90       534\n","\n","Development Set Accuracy for Gradient Boosting: 0.8089887640449438\n","Development Set Classification Report for Gradient Boosting:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.89      0.86       113\n","           1       0.78      0.66      0.72        65\n","\n","    accuracy                           0.81       178\n","   macro avg       0.80      0.78      0.79       178\n","weighted avg       0.81      0.81      0.81       178\n","\n"]}],"source":["# Gradient Boosting model\n","gb_model = GradientBoostingClassifier(random_state=42)\n","train_and_evaluate(gb_model, X_train_scaled, X_dev_scaled, y_train, y_dev, \"Gradient Boosting\")\n"]},{"cell_type":"code","execution_count":349,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best parameters found:  {'max_depth': 10, 'min_samples_leaf': 5, 'n_estimators': 100}\n"]}],"source":["# Perform hyperparameter tuning using GridSearchCV\n","\n","param_grid = {\n","    'max_depth': [5,10,15,20],\n","    'min_samples_leaf': [4,5,6,10],\n","    'n_estimators': [50,100,105],\n","}\n","grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# Retrieve the best parameters\n","best_params = grid_search.best_params_\n","print(\"Best parameters found: \", best_params)\n"]},{"cell_type":"code","execution_count":350,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Set Accuracy for Radnom Forest best tuning: 0.9044943820224719\n","Training Set Classification Report for Radnom Forest best tuning:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.98      0.93       333\n","           1       0.96      0.78      0.86       201\n","\n","    accuracy                           0.90       534\n","   macro avg       0.92      0.88      0.89       534\n","weighted avg       0.91      0.90      0.90       534\n","\n","Development Set Accuracy for Radnom Forest best tuning: 0.8089887640449438\n","Development Set Classification Report for Radnom Forest best tuning\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.89      0.86       113\n","           1       0.78      0.66      0.72        65\n","\n","    accuracy                           0.81       178\n","   macro avg       0.80      0.78      0.79       178\n","weighted avg       0.81      0.81      0.81       178\n","\n"]}],"source":["# fitting the parameters to check the best tuning for Random Forest\n","best_rf = grid_search.best_estimator_\n","best_rf.fit(X_train, y_train)\n","\n","# Predict on the Training set\n","y_train_pred_best = gb_model.predict(X_train_scaled)\n","y_dev_pred_best = gb_model.predict(X_dev_scaled)\n","\n","# Evaluate the model on the development set\n","train_accuracy_best = accuracy_score(y_train, y_train_pred_best)\n","train_report_best = classification_report(y_train, y_train_pred_best)\n","\n","dev_accuracy_best = accuracy_score(y_dev, y_dev_pred_best)\n","dev_report_best = classification_report(y_dev, y_dev_pred_best)\n","\n","print(f\"Training Set Accuracy for Radnom Forest best tuning: {train_accuracy_best}\")\n","print(f\"Training Set Classification Report for Radnom Forest best tuning:\")\n","print(train_report_best)\n","\n","print(f\"Development Set Accuracy for Radnom Forest best tuning: {dev_accuracy_best}\")\n","print(f\"Development Set Classification Report for Radnom Forest best tuning\")\n","print(dev_report_best)"]},{"cell_type":"markdown","metadata":{},"source":["From the results we can summarise that the best model is Gradient Boosting and Radnom Forest with best tuning as they are having the hiest score of the Development data accuracy"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cvML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1904059d3876957b542b45423f2a26c6c4608f5e11cc75420e543fa77f94b066"}}},"nbformat":4,"nbformat_minor":0}
